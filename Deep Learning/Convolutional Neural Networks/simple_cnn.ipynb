{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchviz import make_dot\n",
    "\n",
    "from torchmetrics import Recall, Precision, F1Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset and Dataloader Objects for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "a[2]\n",
    "a.__getitem__(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, data_file, label_file, transform=None):\n",
    "        # Load the data and labels from the CSV files\n",
    "        self.data = np.loadtxt(data_file, delimiter=',')\n",
    "        self.labels = np.loadtxt(label_file, delimiter=',', dtype=np.int64)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Convert the numpy arrays to PyTorch tensors\n",
    "        image = torch.tensor(self.data[index], dtype=torch.float32).reshape(1, 28, 28)  # reshaping to 1x28x28\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.int64)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/__/lcwlgwm95q9_vf5ypxxn3d7c0000gn/T/ipykernel_77786/2623277766.py:5: DeprecationWarning: loadtxt(): Parsing an integer via a float is deprecated.  To avoid this warning, you can:\n",
      "    * make sure the original data is stored as integers.\n",
      "    * use the `converters=` keyword argument.  If you only use\n",
      "      NumPy 1.23 or later, `converters=float` will normally work.\n",
      "    * Use `np.loadtxt(...).astype(np.int64)` parsing the file as\n",
      "      floating point and then convert it.  (On all NumPy versions.)\n",
      "  (Deprecated NumPy 1.23)\n",
      "  self.labels = np.loadtxt(label_file, delimiter=',', dtype=np.int64)\n"
     ]
    }
   ],
   "source": [
    "# Load training and testing data\n",
    "X_train_path = os.path.join('data/MNIST_Original/mnist_train_data.csv')\n",
    "y_train_path = os.path.join('data/MNIST_Original/mnist_train_labels.csv')\n",
    "X_test_path = os.path.join('data/MNIST_Original/mnist_test_data.csv')\n",
    "y_test_path = os.path.join('data/MNIST_Original/mnist_test_labels.csv')\n",
    "\n",
    "train_dataset = MNISTDataset(X_train_path, y_train_path)\n",
    "test_dataset = MNISTDataset(X_test_path, y_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4  # you can change this value based on your requirements\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Datasets\n",
    "\n",
    "Represents a Dataset in pytorch. Defines `__getitem__()` and `__len__()`.\n",
    "- `__getitem__()`: How to access a single data point\n",
    "- `__len__()`: size of the dataset\n",
    "\n",
    "## Torch DataLoader\n",
    "\n",
    "Wraps a `Dataset` object and provides utilities for batching, shuffling, and parallel data loading. \n",
    "\n",
    "It abstracts the complexity of batching and shuffling, providing a clean and efficient way to loop through your dataset in manageable chunks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what an example of a batch looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 28, 28])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([6, 3, 7, 8])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data = next(iter(trainloader))\n",
    "X, y = batch_data[0], batch_data[1]\n",
    "\n",
    "X.shape\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input is a matrix with 4 dimensions!\n",
    "\n",
    "1. 1st dimension (4): number of images in this batch\n",
    "2. 2nd dimension (1): Number of color channels (only one because images are in grey scale)\n",
    "3. 3rd dimension (28): height of image, in pixels\n",
    "4. 4th dimension (28): width of image, in pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does an image look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc61ff29ab0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_to_show = X[0].squeeze()  # Remove the channel dimension, resulting in [28, 28]\n",
    "plt.imshow(image_to_show, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pretty cool right? Let's make this into a function for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image : torch.Tensor):\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build our first CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) # to capture basic patterns from the image\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)# to capture basic patterns from the previous patterns (results in capturing more complex patterns from the original image)\n",
    "        self.fc1 = nn.Linear(320, 50) # 320 = 20 * 4 * 4\n",
    "        self.fc2 = nn.Linear(50, 10) # DNN > WNN; also 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        #return F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a way we can visualize this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model details/model_visualization.png'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 28, 28)  # A dummy input tensor to pass through the model\n",
    "y = model(x)\n",
    "\n",
    "dot = make_dot(y, params=dict(list(model.named_parameters()) + [('x', x)]))\n",
    "dot.render('model details/model_visualization', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741be39661294e5ebf3494e6a8afb3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.420\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/simaonovais/Documents/GitHub/edition-5/Intro to Deep Learning/Convolutional Neural Networks/simple_cnn.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simaonovais/Documents/GitHub/edition-5/Intro%20to%20Deep%20Learning/Convolutional%20Neural%20Networks/simple_cnn.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)  \u001b[39m# calculate loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simaonovais/Documents/GitHub/edition-5/Intro%20to%20Deep%20Learning/Convolutional%20Neural%20Networks/simple_cnn.ipynb#X26sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()  \u001b[39m# backward\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/simaonovais/Documents/GitHub/edition-5/Intro%20to%20Deep%20Learning/Convolutional%20Neural%20Networks/simple_cnn.ipynb#X26sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()  \u001b[39m# optimize\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simaonovais/Documents/GitHub/edition-5/Intro%20to%20Deep%20Learning/Convolutional%20Neural%20Networks/simple_cnn.ipynb#X26sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simaonovais/Documents/GitHub/edition-5/Intro%20to%20Deep%20Learning/Convolutional%20Neural%20Networks/simple_cnn.ipynb#X26sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m2000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m1999\u001b[39m:  \u001b[39m# print every 2000 mini-batches\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/sgd.py:75\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     71\u001b[0m momentum_buffer_list \u001b[39m=\u001b[39m []\n\u001b[1;32m     73\u001b[0m has_sparse_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[0;32m---> 75\u001b[0m sgd(params_with_grad,\n\u001b[1;32m     76\u001b[0m     d_p_list,\n\u001b[1;32m     77\u001b[0m     momentum_buffer_list,\n\u001b[1;32m     78\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     79\u001b[0m     momentum\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmomentum\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     80\u001b[0m     lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     81\u001b[0m     dampening\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdampening\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     82\u001b[0m     nesterov\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mnesterov\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     83\u001b[0m     maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     84\u001b[0m     has_sparse_grad\u001b[39m=\u001b[39;49mhas_sparse_grad,\n\u001b[1;32m     85\u001b[0m     foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     87\u001b[0m \u001b[39m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39mfor\u001b[39;00m p, momentum_buffer \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/sgd.py:208\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mif\u001b[39;00m foreach \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[39m# why must we be explicit about an if statement for torch.jit.is_scripting here?\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[39m# because JIT can't handle Optionals nor fancy conditionals when scripting\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting():\n\u001b[0;32m--> 208\u001b[0m         _, foreach \u001b[39m=\u001b[39m _default_to_fused_or_foreach(params, differentiable\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, use_fused\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         foreach \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:122\u001b[0m, in \u001b[0;36m_default_to_fused_or_foreach\u001b[0;34m(params, differentiable, use_fused)\u001b[0m\n\u001b[1;32m    116\u001b[0m foreach_supported_devices \u001b[39m=\u001b[39m _get_foreach_kernels_supported_devices()\n\u001b[1;32m    117\u001b[0m fused \u001b[39m=\u001b[39m use_fused \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m    118\u001b[0m     p \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m (\u001b[39mtype\u001b[39m(p) \u001b[39min\u001b[39;00m _foreach_supported_types \u001b[39mand\u001b[39;00m\n\u001b[1;32m    119\u001b[0m                   p\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39min\u001b[39;00m fused_supported_devices \u001b[39mand\u001b[39;00m\n\u001b[1;32m    120\u001b[0m                   torch\u001b[39m.\u001b[39mis_floating_point(p)) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m params\n\u001b[1;32m    121\u001b[0m )\n\u001b[0;32m--> 122\u001b[0m foreach \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m fused \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39;49m(\n\u001b[1;32m    123\u001b[0m     p \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39mor\u001b[39;49;00m (\u001b[39mtype\u001b[39;49m(p) \u001b[39min\u001b[39;49;00m _foreach_supported_types \u001b[39mand\u001b[39;49;00m\n\u001b[1;32m    124\u001b[0m                   p\u001b[39m.\u001b[39;49mdevice\u001b[39m.\u001b[39;49mtype \u001b[39min\u001b[39;49;00m foreach_supported_devices) \u001b[39mfor\u001b[39;49;00m p \u001b[39min\u001b[39;49;00m params\n\u001b[1;32m    125\u001b[0m )\n\u001b[1;32m    126\u001b[0m \u001b[39mreturn\u001b[39;00m fused, foreach\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 1: Define the model\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Step 2: Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0003, momentum=0.9)\n",
    "\n",
    "\n",
    "# Step 3: Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in tqdm(list(range(num_epochs))):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()  # zero the parameter gradients\n",
    "        outputs = model(inputs)  # forward\n",
    "        loss = criterion(outputs, labels)  # calculate loss\n",
    "        loss.backward()  # backward\n",
    "        optimizer.step()  # optimize\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to interrupt training if you feel like the model is not improving anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2d576a05cd4255a06f374e587f9e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on the train set: 0.047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990679ef80614dbdba5c40d7287181b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on the test set: 0.064\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Evaluation\n",
    "\n",
    "# What's the loss on the train set?\n",
    "total_loss = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(trainloader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()  # zero the parameter gradients\n",
    "        outputs = model(inputs)  # forward\n",
    "        loss = criterion(outputs, labels)  # calculate loss\n",
    "        total_loss.append(loss.item())\n",
    "print('Loss on the train set: %.3f' % (sum(total_loss) / len(total_loss)))\n",
    "\n",
    "\n",
    "# What's the loss on the test set?\n",
    "total_loss = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(testloader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()  # zero the parameter gradients\n",
    "        outputs = model(inputs)  # forward\n",
    "        loss = criterion(outputs, labels)  # calculate loss\n",
    "        total_loss.append(loss.item())\n",
    "print('Loss on the test set: %.3f' % (sum(total_loss) / len(total_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok might be overfitting a bit but it's not too bad. Now let's look at more interesting metrics like recall and precision.\n",
    "\n",
    "For these metrics we require the model decisions, but our current model returns only the logits (the output of the last Linear Layer). So let's run this over a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7f9705c9fb4d909239e0fcd1e92c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on the test set: 0.98\n",
      "Precision on the test set: 0.98\n",
      "F1 Score on the test set: 0.98\n"
     ]
    }
   ],
   "source": [
    "recall = Recall(task='multiclass',num_classes=10)\n",
    "precision = Precision(task='multiclass',num_classes=10)\n",
    "f1 = F1Score(task='multiclass',num_classes=10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(testloader):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)  # forward\n",
    "        \n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        recall.update(predictions, labels)\n",
    "        precision.update(predictions, labels)\n",
    "        f1.update(predictions, labels)\n",
    "\n",
    "print('Recall on the test set: %.2f' % (recall.compute()))\n",
    "print('Precision on the test set: %.2f' % (precision.compute()))\n",
    "print('F1 Score on the test set: %.2f' % (f1.compute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Want to save a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving models is very useful:\n",
    "1. share with others\n",
    "2. don't have to train again next time\n",
    "3. Create model checkpoints to:\n",
    "    - prevent loss from interruption\n",
    "    - limited access to computing resources\n",
    "    - debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pytorch model\n",
    "torch.save(model.state_dict(), 'models/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to load a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read pytorch model\n",
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load('models/model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is this model still good at prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7221c779971a4364bca6f0ad1b104b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on the test set: 0.99\n",
      "Precision on the test set: 0.99\n",
      "F1 Score on the test set: 0.99\n"
     ]
    }
   ],
   "source": [
    "recall = Recall(task='multiclass',num_classes=10)\n",
    "precision = Precision(task='multiclass',num_classes=10)\n",
    "f1 = F1Score(task='multiclass',num_classes=10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(testloader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()  # zero the parameter gradients\n",
    "        outputs = model(inputs)  # forward\n",
    "        \n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        recall.update(predictions, labels)\n",
    "        precision.update(predictions, labels)\n",
    "        f1.update(predictions, labels)\n",
    "\n",
    "print('Recall on the test set: %.2f' % (recall.compute()))\n",
    "print('Precision on the test set: %.2f' % (precision.compute()))\n",
    "print('F1 Score on the test set: %.2f' % (f1.compute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could package some of this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_evaluation(model, dataloader, criterion):\n",
    "    \"\"\"\n",
    "    Our custom evaluation of a model for this particular task and dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    recall = Recall(task='multiclass',num_classes=10)\n",
    "    precision = Precision(task='multiclass',num_classes=10)\n",
    "    f1 = F1Score(task='multiclass',num_classes=10)\n",
    "    total_loss = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)  # forward\n",
    "            loss = criterion(outputs, labels)  # calculate loss\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            recall.update(predictions, labels)\n",
    "            precision.update(predictions, labels)\n",
    "            f1.update(predictions, labels)\n",
    "\n",
    "    average_loss = sum(total_loss) / len(total_loss)\n",
    "                \n",
    "    return {'loss':average_loss, \n",
    "            'recall':recall.compute().item(), \n",
    "            'precision':precision.compute().item(), \n",
    "            'f1':f1.compute().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f688e39e8d1d418a83c034c3c82dd8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 24.74197465057373,\n",
       " 'recall': 0.09910000115633011,\n",
       " 'precision': 0.09910000115633011,\n",
       " 'f1': 0.09910000115633011}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = SimpleCNN()\n",
    "dataloader_evaluation(model2,testloader,criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for the training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, print_every=2000):\n",
    "    \"\"\"\n",
    "    Our custom evaluation of a model for this particular task and dataset.\n",
    "    \"\"\"\n",
    "    running_loss = []\n",
    "    for i, data in tqdm(enumerate(dataloader, 0)):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()  # zero the parameter gradients\n",
    "        outputs = model(inputs)  # forward\n",
    "        loss = criterion(outputs, labels)  # calculate loss\n",
    "        loss.backward()  # backward\n",
    "        optimizer.step()  # optimize\n",
    "\n",
    "        running_loss.append(loss)\n",
    "\n",
    "        if i % print_every == print_every-1:  # print every 2000 mini-batches\n",
    "            print('[%5d] loss: %.3f' % (i + 1, loss.item()))\n",
    "    return torch.mean(loss).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a579b5a34d452398e5fbc3b457f2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2000] loss: 0.000\n",
      "[ 4000] loss: 0.000\n",
      "[ 6000] loss: 0.000\n",
      "[ 8000] loss: 0.000\n",
      "[10000] loss: 0.000\n",
      "[12000] loss: 0.000\n",
      "[14000] loss: 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_one_epoch(model, trainloader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So now we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in tqdm(list(range(2))):\n",
    "    train_one_epoch(model, trainloader, criterion, optimizer)\n",
    "    evaluation = dataloader_evaluation(model, testloader, criterion)\n",
    "    print('Epoch %d: loss=%.3f, recall=%.2f, precision=%.2f, f1=%.2f' % (epoch+1, evaluation['loss'], evaluation['recall'], evaluation['precision'], evaluation['f1']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
