{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.7)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence model with RNN encoder and decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to begin implementing our first sequence to sequence model. We will do this using a RNN encoder and a RNN decoder. \n",
    "\n",
    "The encoder will take a source sentence as input and will encode it into a single vector (also known as a context vector or a latent vector) which will be passed to the decoder. The decoder will then use this context vector to generate a new sequence (in our case, a translation of the source sentence).\n",
    "\n",
    "So, given a sequential input sentence $X = \\{x_1, x_2, ..., x_T\\}$, we want to encode it into a single vector $z$. At each step we have the hidden state $h_t$:\n",
    "<br>\n",
    "<br>\n",
    "$$h_t = \\text{Encoder}(e(x_t), h_{t-1}),$$\n",
    "<br>\n",
    "where $e(x_t)$ is the embedding of the current token $x_t$. This means that in practive, the $z$ vector will actually be $h_T$, the last hidden state of the RNN.\n",
    "\n",
    "After this, we will use $z$ as the initial hidden state of the decoder. The decoder works just like the encoder, except that it takes $z$ as the initial hidden state and takes as input the previous token embedding $e(y_t)$ and the previous hidden state $s_{t-1}$ to predict the next hidden state $s_t$:\n",
    "<br>\n",
    "<br>\n",
    "$$s_t = \\text{Decoder}(d(y_t), s_{t-1}),$$\n",
    "<br>\n",
    "\n",
    "where $d(y_t)$ is the embedding of the current token $y_t$. We use another letter because the embedding of the input and output tokens may be different. Finally, we use the hidden state $s_t$ to predict the next token $\\hat{y}_{t+1}$. We do this until we predict an end-of-sentence token, or we reach a maximum length of the sequence. Follows a diagram of this process (the examples uses a GRU):\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://incredible.ai/assets/images/seq2seq-seq2seq_ts.png\" />\n",
    "</p>\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will the dataset named \"VanessaSchenkel/translation-en-pt\", available in HuggingFace's datasets library. This dataset contains pairs of sentences in English and Portuguese. We will use this dataset to train our model to translate from English to Portuguese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "main_data = load_dataset(\"VanessaSchenkel/translation-en-pt\", field=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation', 'id'],\n",
       "        num_rows: 260482\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains only a train split, so we will split it into train and validation sets. We will use 80% of the data for training and 20% for validation. \n",
    "\n",
    "Like before we first need to pre-process the data and tokenize all examples, encode them into integers and create dataloaders to iterate over the batches. \n",
    "\n",
    "We will use the same tokenizer as before, but we will need to add the special tokens \"\\<eos\\>\" (end of sentence) and \"\\<sos\\>\" (start of sentence) to the vocabulary. \n",
    "\n",
    "We start by building the two vocabulary, one for the source language (English) and one for the target language (Portuguese).\n",
    "\n",
    "Note: we limit the total number of exemples to 50000 to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocabulary size:  11461\n",
      "Portuguese vocabulary size:  17554\n",
      "\n",
      "Maximum length of English sentences:  15\n",
      "Maximum length of Portuguese sentences:  15\n"
     ]
    }
   ],
   "source": [
    " # For this example we will keep punctuation and capital letters, meaning that can use directly the word_tokenize function from nltk\n",
    "# Also, we will not remove stopwords or rare words, since they can be important for the translation\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# note: skip the example with id 199351, since it is a very long sentence\n",
    "english_tokens= []\n",
    "portuguese_tokens = []\n",
    "for d in main_data['train']:\n",
    "    eng_tokens = word_tokenize(d['translation']['english'].lower())\n",
    "    pt_tokens = word_tokenize(d['translation']['portuguese'].lower())\n",
    "    if len(eng_tokens) > 15 or len(pt_tokens) > 15:\n",
    "        continue\n",
    "    english_tokens.append(eng_tokens)\n",
    "    portuguese_tokens.append(pt_tokens)\n",
    "    if len(english_tokens) == 50000:\n",
    "        break\n",
    "\n",
    "# Is 15 but let's get the maximum length of the sentences. We will use this to pad the sentences\n",
    "max_len_english = max([len(s) for s in english_tokens])\n",
    "max_len_portuguese = max([len(s) for s in portuguese_tokens])\n",
    "\n",
    "# Ok, now we can get the unique tokens for each language\n",
    "unique_english_tokens = sorted(list(set([tk for s in english_tokens for tk in s])))\n",
    "unique_portuguese_tokens = sorted(list(set([tk for s in portuguese_tokens for tk in s])))\n",
    "\n",
    "print(\"English vocabulary size: \", len(unique_english_tokens))\n",
    "print(\"Portuguese vocabulary size: \", len(unique_portuguese_tokens))\n",
    "print(\"\")\n",
    "print(\"Maximum length of English sentences: \", max_len_english)\n",
    "print(\"Maximum length of Portuguese sentences: \", max_len_portuguese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['let', \"'s\", 'try', 'something', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_english_tokens = ['<pad>','<sos>', '<eos>'] + unique_english_tokens\n",
    "tokeng2id = {t: i for i, t in enumerate(unique_english_tokens)}\n",
    "id2tokeng = {i: t for t, i in tokeng2id.items()}\n",
    "\n",
    "unique_portuguese_tokens = ['<pad>','<sos>', '<eos>'] + unique_portuguese_tokens\n",
    "tokpt2id = {t: i for i, t in enumerate(unique_portuguese_tokens)}\n",
    "id2tokpt = {i: t for t, i in tokpt2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(tokeng2id[\"<pad>\"])\n",
    "print(tokeng2id[\"<sos>\"])\n",
    "print(tokeng2id[\"<eos>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things simpler let's add the special tokens manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokens_ids = [[1]+[tokeng2id[t] for t in s]+[2] for s in english_tokens]\n",
    "portuguese_tokens_ids = [[1]+[tokpt2id[t] for t in s]+[2] for s in portuguese_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pad each sentence to the maximum length of the batch. This means that if the maximum length of the batch is 50, all sentences will be padded to length 50. \n",
    "\n",
    "We will the english sentences to the left because we want the final token to be the \\<eos\\>, and we will pad the portuguese sentences to the right because we want the first token to be the \\<sos\\>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(seq, max_length = 500, pad_direction = 'left'):\n",
    "    if pad_direction == 'left':\n",
    "        return seq[:max_length] if len(seq) > max_length else [0] * (max_length - len(seq)) + seq\n",
    "    elif pad_direction == 'right':\n",
    "        return seq[:max_length] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
    "    else:\n",
    "        raise ValueError(\"pad_direction must be either 'left' or 'right'\")\n",
    "\n",
    "\n",
    "english_tokens_ids = [pad_sequence(seq, max_length=17,pad_direction='left') for seq in english_tokens_ids]\n",
    "portuguese_tokens_ids = [pad_sequence(seq, max_length=17,pad_direction='right') for seq in portuguese_tokens_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6057, 24, 10606, 9529, 29, 2]\n",
      "[1, 16811, 16005, 1005, 3647, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(english_tokens_ids[0])\n",
    "print(portuguese_tokens_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader\n",
    "\n",
    "Ok, we are now ready to create the dataset and the dataloaders. We will use the same batch size as before (32).\n",
    "Also, before we need to do the split between train and validation sets. We will use 80% of the data for training and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set:  40000\n",
      "Size of validation set:  10000\n"
     ]
    }
   ],
   "source": [
    "# We assume the data is already randomly shuffled\n",
    "train_size = int(len(english_tokens_ids) * 0.8)\n",
    "\n",
    "train_en = english_tokens_ids[:train_size]\n",
    "train_pt = portuguese_tokens_ids[:train_size]\n",
    "\n",
    "val_en = english_tokens_ids[train_size:]\n",
    "val_pt = portuguese_tokens_ids[train_size:]\n",
    "\n",
    "print(\"Size of training set: \", len(train_en))\n",
    "print(\"Size of validation set: \", len(val_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we can create the Dataloader with the help of the Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "\n",
    "list_data = [{'english':train_en[i],'portuguese':train_pt[i]} for i in range(len(train_en))]\n",
    "train_dataset = Dataset.from_list(list_data)\n",
    "train_dataset = train_dataset.with_format(\"torch\")\n",
    "\n",
    "batch_size = 32 # number of sequences in each batch\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size) # train_dataloader is an iterator that returns a batch each time it is called\n",
    "\n",
    "list_data = [{'english':val_en[i],'portuguese':val_pt[i]} for i in range(len(val_en))]\n",
    "val_dataset = Dataset.from_list(list_data)\n",
    "val_dataset = val_dataset.with_format(\"torch\")\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size) # val_dataloader is an iterator that returns a batch each time it is called"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder, Decoder \n",
    "\n",
    "We are now ready to implement the encoder, decoder and seq2seq models. We will use a LSTM for both the encoder and the decoder.\n",
    "\n",
    "Let's start with the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4055522170>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main class of the RNN built from the nn.Module class\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_d,hidden_d,n_layers, drop_prob = 0.5):\n",
    "        \"\"\"\n",
    "        Initialize the RNN Module\n",
    "\n",
    "        Arguments:\n",
    "        vocab_size: size of the vocabulary\n",
    "        output_size: size of the output layer\n",
    "        emb_d: size of the embedding layer\n",
    "        h_d: size of the hidden layer\n",
    "        n_layers: number of layers\n",
    "        drop_prob: dropout probability\n",
    "        \"\"\"\n",
    "    \n",
    "        super().__init__()\n",
    "\n",
    "        # define the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_d)\n",
    "\n",
    "        # define a RNN layer\n",
    "        self.rnn = nn.LSTM(emb_d, hidden_d, n_layers, dropout = drop_prob, batch_first=True) # batch_first=True means that the first dimension of the input and output will be the batch_size\n",
    "\n",
    "        # define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "\n",
    "        Arguments:\n",
    "        x: input to the model\n",
    "        hidden: hidden state\n",
    "\n",
    "        Returns:\n",
    "        output: output of the model\n",
    "        hidden: hidden state\n",
    "        \"\"\"\n",
    "\n",
    "        # get the embedding vectors from lookup embedding layer \n",
    "        embeds = self.embedding(x) # shape: (batch_size, seq_length, emb_d)\n",
    "\n",
    "        # pass the embedding vectors to the RNN layer. We get the output and the hidden state and cell state to initialize the decoder  \n",
    "        # shape of out: (batch_size, seq_length, hidden_d)\n",
    "        # shape of hidden: (n_layers, batch_size, hidden_d)\n",
    "        # shape of cell: (n_layers, batch_size, hidden_d)\n",
    "        out, (hidden,cell) = self.rnn(embeds) \n",
    "        out = self.dropout(out)\n",
    "        return out, (hidden,cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the Encoder with a forward pass on the first english example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/env/lib/python3.9/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embedding): Embedding(11464, 50)\n",
       "  (rnn): LSTM(50, 32, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokeng2id)\n",
    "embedding_dim = 50\n",
    "hidden_dim = 32\n",
    "n_layers = 1\n",
    "\n",
    "model_enc = Encoder(vocab_size, embedding_dim, hidden_dim, n_layers)\n",
    "model_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['let', \"'s\", 'try', 'something', '.']\n",
      "Tokens IDS: [6057, 24, 10606, 9529, 29]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens:\",english_tokens[0])\n",
    "token_ids = [tokeng2id[tk] for tk in english_tokens[0]]\n",
    "print(\"Tokens IDS:\",token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.IntTensor([token_ids,token_ids]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 5, 32])\n",
      "Hidden shape: torch.Size([1, 2, 32])\n",
      "Cell shape: torch.Size([1, 2, 32])\n",
      "tensor([[[ 1.3423e-01,  3.6583e-02, -6.6397e-02,  2.2418e-01, -1.1520e-01,\n",
      "           3.4336e-01,  4.3391e-02, -1.7703e-02, -1.5032e-01,  1.1651e-01,\n",
      "          -7.8157e-02, -2.1796e-02, -3.2822e-02,  5.7263e-02, -1.1593e-01,\n",
      "           1.0967e-02, -8.7867e-02,  6.7248e-02,  2.9072e-02, -1.2149e-01,\n",
      "          -3.6983e-02, -1.4534e-02,  5.6360e-02,  7.8790e-02, -1.8293e-02,\n",
      "           4.6104e-02, -2.6533e-02,  5.0323e-02, -4.1733e-02, -1.6648e-01,\n",
      "          -7.0855e-02,  2.6974e-01],\n",
      "         [ 1.5144e-02, -5.9152e-02, -1.7571e-01,  1.2108e-01, -4.8443e-03,\n",
      "           2.9913e-01,  1.3320e-01,  7.6689e-02, -2.6027e-01,  1.7537e-01,\n",
      "          -3.3836e-02,  1.8307e-01,  7.0443e-02, -1.5437e-01, -2.1981e-01,\n",
      "          -2.2986e-01,  1.9333e-01,  1.4743e-01, -2.6945e-01,  1.1514e-02,\n",
      "          -6.5978e-02, -7.5610e-02,  4.8579e-01, -3.4326e-02,  1.2818e-01,\n",
      "          -6.8017e-04, -6.0655e-02,  7.9515e-02, -7.7247e-02, -4.8174e-02,\n",
      "           9.1363e-02,  1.2911e-01],\n",
      "         [ 9.7390e-02, -1.6086e-01, -3.1452e-02, -6.1340e-02, -6.1099e-02,\n",
      "           3.9543e-01, -1.1182e-01,  1.1725e-01, -1.7299e-01, -1.3617e-01,\n",
      "          -7.2085e-02,  1.4673e-01,  1.7721e-01, -2.2077e-01, -8.3372e-02,\n",
      "          -1.9699e-01, -3.2424e-01,  2.3713e-01,  5.0446e-04, -4.0099e-03,\n",
      "          -2.8781e-01, -7.9360e-02,  2.9318e-01, -1.2443e-01,  5.1191e-02,\n",
      "          -1.1325e-02, -1.4683e-01,  7.7491e-02, -2.2769e-01, -3.1910e-03,\n",
      "           8.2609e-02,  4.6233e-02],\n",
      "         [ 6.5446e-02, -9.4494e-02,  7.0010e-02,  1.0192e-02, -5.6911e-02,\n",
      "           2.2295e-01,  9.7960e-02,  1.2865e-02, -1.8286e-01,  6.1446e-03,\n",
      "          -2.4893e-01,  4.5966e-01,  3.8542e-01,  4.4974e-02, -9.3217e-02,\n",
      "           1.1452e-03,  9.5231e-02,  1.2305e-01, -1.9780e-01, -2.4199e-01,\n",
      "          -6.8069e-02, -1.0377e-01,  2.7988e-01,  4.3321e-02,  1.0383e-01,\n",
      "           2.5201e-01, -2.3861e-01, -9.6088e-03,  1.1930e-01,  5.8653e-03,\n",
      "           2.4421e-02, -2.4532e-02],\n",
      "         [ 3.4702e-02, -4.3110e-02, -1.0624e-01, -1.4382e-02,  5.4988e-02,\n",
      "           1.1388e-01,  2.4860e-01,  9.1572e-02, -5.1602e-01,  1.0284e-01,\n",
      "          -2.8787e-01,  2.7854e-01,  2.8952e-01, -1.0350e-01,  1.1133e-01,\n",
      "          -1.1385e-01,  1.9210e-01,  5.7828e-02, -3.5966e-01,  1.2179e-01,\n",
      "          -1.4378e-01, -1.4685e-01,  2.1278e-01, -1.0170e-01, -3.3697e-02,\n",
      "           1.5098e-01, -1.3188e-01, -1.0725e-01,  7.3213e-02,  2.0808e-01,\n",
      "           1.1028e-01, -5.7981e-02]],\n",
      "\n",
      "        [[ 1.3423e-01,  3.6583e-02, -6.6397e-02,  2.2418e-01, -1.1520e-01,\n",
      "           3.4336e-01,  4.3391e-02, -1.7703e-02, -1.5032e-01,  1.1651e-01,\n",
      "          -7.8157e-02, -2.1796e-02, -3.2822e-02,  5.7263e-02, -1.1593e-01,\n",
      "           1.0967e-02, -8.7867e-02,  6.7248e-02,  2.9072e-02, -1.2149e-01,\n",
      "          -3.6983e-02, -1.4534e-02,  5.6360e-02,  7.8790e-02, -1.8293e-02,\n",
      "           4.6104e-02, -2.6533e-02,  5.0323e-02, -4.1733e-02, -1.6648e-01,\n",
      "          -7.0855e-02,  2.6974e-01],\n",
      "         [ 1.5144e-02, -5.9152e-02, -1.7571e-01,  1.2108e-01, -4.8443e-03,\n",
      "           2.9913e-01,  1.3320e-01,  7.6689e-02, -2.6027e-01,  1.7537e-01,\n",
      "          -3.3836e-02,  1.8307e-01,  7.0443e-02, -1.5437e-01, -2.1981e-01,\n",
      "          -2.2986e-01,  1.9333e-01,  1.4743e-01, -2.6945e-01,  1.1514e-02,\n",
      "          -6.5978e-02, -7.5610e-02,  4.8579e-01, -3.4326e-02,  1.2818e-01,\n",
      "          -6.8017e-04, -6.0655e-02,  7.9515e-02, -7.7247e-02, -4.8174e-02,\n",
      "           9.1363e-02,  1.2911e-01],\n",
      "         [ 9.7390e-02, -1.6086e-01, -3.1452e-02, -6.1340e-02, -6.1099e-02,\n",
      "           3.9543e-01, -1.1182e-01,  1.1725e-01, -1.7299e-01, -1.3617e-01,\n",
      "          -7.2085e-02,  1.4673e-01,  1.7721e-01, -2.2077e-01, -8.3372e-02,\n",
      "          -1.9699e-01, -3.2424e-01,  2.3713e-01,  5.0446e-04, -4.0099e-03,\n",
      "          -2.8781e-01, -7.9360e-02,  2.9318e-01, -1.2443e-01,  5.1191e-02,\n",
      "          -1.1325e-02, -1.4683e-01,  7.7491e-02, -2.2769e-01, -3.1910e-03,\n",
      "           8.2609e-02,  4.6233e-02],\n",
      "         [ 6.5446e-02, -9.4494e-02,  7.0010e-02,  1.0192e-02, -5.6911e-02,\n",
      "           2.2295e-01,  9.7960e-02,  1.2865e-02, -1.8286e-01,  6.1446e-03,\n",
      "          -2.4893e-01,  4.5966e-01,  3.8542e-01,  4.4974e-02, -9.3217e-02,\n",
      "           1.1452e-03,  9.5231e-02,  1.2305e-01, -1.9780e-01, -2.4199e-01,\n",
      "          -6.8069e-02, -1.0377e-01,  2.7988e-01,  4.3321e-02,  1.0383e-01,\n",
      "           2.5201e-01, -2.3861e-01, -9.6088e-03,  1.1930e-01,  5.8653e-03,\n",
      "           2.4421e-02, -2.4532e-02],\n",
      "         [ 3.4702e-02, -4.3110e-02, -1.0624e-01, -1.4382e-02,  5.4988e-02,\n",
      "           1.1388e-01,  2.4860e-01,  9.1572e-02, -5.1602e-01,  1.0284e-01,\n",
      "          -2.8787e-01,  2.7854e-01,  2.8952e-01, -1.0350e-01,  1.1133e-01,\n",
      "          -1.1385e-01,  1.9210e-01,  5.7828e-02, -3.5966e-01,  1.2179e-01,\n",
      "          -1.4378e-01, -1.4685e-01,  2.1278e-01, -1.0170e-01, -3.3697e-02,\n",
      "           1.5098e-01, -1.3188e-01, -1.0725e-01,  7.3213e-02,  2.0808e-01,\n",
      "           1.1028e-01, -5.7981e-02]]], grad_fn=<TransposeBackward0>)\n",
      "tensor([[[ 0.0347, -0.0431, -0.1062, -0.0144,  0.0550,  0.1139,  0.2486,\n",
      "           0.0916, -0.5160,  0.1028, -0.2879,  0.2785,  0.2895, -0.1035,\n",
      "           0.1113, -0.1139,  0.1921,  0.0578, -0.3597,  0.1218, -0.1438,\n",
      "          -0.1469,  0.2128, -0.1017, -0.0337,  0.1510, -0.1319, -0.1072,\n",
      "           0.0732,  0.2081,  0.1103, -0.0580],\n",
      "         [ 0.0347, -0.0431, -0.1062, -0.0144,  0.0550,  0.1139,  0.2486,\n",
      "           0.0916, -0.5160,  0.1028, -0.2879,  0.2785,  0.2895, -0.1035,\n",
      "           0.1113, -0.1139,  0.1921,  0.0578, -0.3597,  0.1218, -0.1438,\n",
      "          -0.1469,  0.2128, -0.1017, -0.0337,  0.1510, -0.1319, -0.1072,\n",
      "           0.0732,  0.2081,  0.1103, -0.0580]]], grad_fn=<StackBackward0>)\n",
      "tensor([[[ 0.2055, -0.1139, -0.3166, -0.0376,  0.2563,  0.2496,  0.4189,\n",
      "           0.2883, -0.8187,  0.3130, -0.7555,  0.3708,  0.6079, -0.5705,\n",
      "           0.1868, -0.1566,  0.2841,  0.0810, -0.6309,  0.2858, -0.2823,\n",
      "          -0.2980,  0.2762, -0.1307, -0.1094,  0.7242, -0.3728, -0.1997,\n",
      "           0.2707,  0.3920,  0.2033, -0.2117],\n",
      "         [ 0.2055, -0.1139, -0.3166, -0.0376,  0.2563,  0.2496,  0.4189,\n",
      "           0.2883, -0.8187,  0.3130, -0.7555,  0.3708,  0.6079, -0.5705,\n",
      "           0.1868, -0.1566,  0.2841,  0.0810, -0.6309,  0.2858, -0.2823,\n",
      "          -0.2980,  0.2762, -0.1307, -0.1094,  0.7242, -0.3728, -0.1997,\n",
      "           0.2707,  0.3920,  0.2033, -0.2117]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_enc.eval()\n",
    "output_enc,(hidden_z,cell_z) = model_enc(torch.IntTensor([token_ids,token_ids])) # We had two sentences just to check simulate a batch of size 2\n",
    "print(\"Output shape:\",output_enc.shape)\n",
    "print(\"Hidden shape:\",hidden_z.shape)\n",
    "print(\"Cell shape:\",cell_z.shape)\n",
    "print(output_enc)\n",
    "print(hidden_z)\n",
    "print(cell_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder seems to be working fine. It outputs 5 hidden states, one for each token in the input sequenc, the last hidden state (that is equal to the last output)  and the cell state.\n",
    "\n",
    "Now let's implement the decoder. The decoder will take as input the encoder hidden state and cell state, and the target sequence (t-1) and will output the predicted sequence (t). We will also add a fully connected layer to the output of the decoder to predict the next token.\n",
    "\n",
    "Note: the encoder the RNN also takes the hidden and cell states but we don't need to pass them explicitly because they are initialized to zero by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_d,hidden_d,n_layers, drop_prob = 0.5):\n",
    "        \"\"\"\n",
    "        Initialize the RNN Module\n",
    "\n",
    "        Arguments:\n",
    "        vocab_size: size of the vocabulary\n",
    "        output_size: size of the output layer\n",
    "        emb_d: size of the embedding layer\n",
    "        h_d: size of the hidden layer\n",
    "        n_layers: number of layers\n",
    "        drop_prob: dropout probability\n",
    "        \"\"\"\n",
    "    \n",
    "        super().__init__()\n",
    "\n",
    "        # define the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_d)\n",
    "\n",
    "        # define a RNN layer\n",
    "        self.rnn = nn.LSTM(emb_d, hidden_d, n_layers, dropout = drop_prob, batch_first=True) # batch_first=True means that the first dimension of the input and output will be the batch_size\n",
    "\n",
    "        # define a linear layer what will be used to predict the next word\n",
    "        self.fc_out = nn.Linear(hidden_d, vocab_size)\n",
    "\n",
    "        # define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "\n",
    "        Arguments:\n",
    "        x: input to the model\n",
    "        hidden: hidden state\n",
    "        cell: cell state\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        # get the embedding vectors from lookup embedding layer \n",
    "        embeds = self.embedding(x)\n",
    "\n",
    "        # pass the embedding vectors, the hidden can cell states to the RNN layer.\n",
    "        # contrary to the encoder, the hidden and cell states are not initialized with zeros, but with the values from the encoder\n",
    "        # shape of out: (batch_size, seq_length, hidden_d)\n",
    "        # shape of hidden: (n_layers, batch_size, hidden_d)\n",
    "        # shape of cell: (n_layers, batch_size, hidden_d)\n",
    "        out, (hidden, cell) = self.rnn(embeds, (hidden, cell))\n",
    "\n",
    "        out = self.dropout(out)\n",
    "\n",
    "\n",
    "        # fully connected layer that will return a vector with the size of the vocabulary. This vector will be used to predict the next word\n",
    "        # shape of pred: (batch_size, seq_length, vocab_size)\n",
    "        pred = self.fc_out(out)\n",
    "\n",
    "        return pred, (hidden, cell)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok let's check if the decoder is working using the context vectors we got from the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embedding): Embedding(11464, 50)\n",
       "  (rnn): LSTM(50, 32, batch_first=True, dropout=0.5)\n",
       "  (fc_out): Linear(in_features=32, out_features=11464, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dec = Decoder(vocab_size, embedding_dim, hidden_dim, n_layers)\n",
    "model_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test with a forward pass on the first english example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 1, 11464])\n",
      "Hidden shape: torch.Size([1, 2, 32])\n",
      "Cell shape: torch.Size([1, 2, 32])\n",
      "tensor([[[ 0.0288, -0.0730, -0.0555,  ..., -0.1163, -0.0195,  0.0394]],\n",
      "\n",
      "        [[ 0.0653, -0.1216,  0.0790,  ..., -0.1256, -0.1290,  0.1666]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.1121, -0.0330,  0.0318, -0.1038,  0.0981, -0.1162, -0.0084,\n",
      "           0.1633,  0.0326,  0.0095, -0.1046,  0.1206,  0.0851, -0.2097,\n",
      "           0.0712,  0.0960,  0.2367,  0.0634,  0.0012, -0.0975,  0.0025,\n",
      "          -0.1637,  0.0416,  0.1224,  0.1393,  0.0629, -0.1018, -0.0705,\n",
      "           0.2856,  0.2441, -0.1941,  0.0114],\n",
      "         [-0.1121, -0.0330,  0.0318, -0.1038,  0.0981, -0.1162, -0.0084,\n",
      "           0.1633,  0.0326,  0.0095, -0.1046,  0.1206,  0.0851, -0.2097,\n",
      "           0.0712,  0.0960,  0.2367,  0.0634,  0.0012, -0.0975,  0.0025,\n",
      "          -0.1637,  0.0416,  0.1224,  0.1393,  0.0629, -0.1018, -0.0705,\n",
      "           0.2856,  0.2441, -0.1941,  0.0114]]], grad_fn=<StackBackward0>)\n",
      "tensor([[[-0.2419, -0.0891,  0.1196, -0.3629,  0.3125, -0.3737, -0.0185,\n",
      "           0.3872,  0.1299,  0.0172, -0.3369,  0.3034,  0.1778, -0.4361,\n",
      "           0.1535,  0.1526,  0.3617,  0.1178,  0.0035, -0.3033,  0.0035,\n",
      "          -0.3915,  0.0737,  0.3588,  0.1897,  0.2910, -0.4500, -0.1561,\n",
      "           0.3926,  0.4045, -0.3430,  0.0588],\n",
      "         [-0.2419, -0.0891,  0.1196, -0.3629,  0.3125, -0.3737, -0.0185,\n",
      "           0.3872,  0.1299,  0.0172, -0.3369,  0.3034,  0.1778, -0.4361,\n",
      "           0.1535,  0.1526,  0.3617,  0.1178,  0.0035, -0.3033,  0.0035,\n",
      "          -0.3915,  0.0737,  0.3588,  0.1897,  0.2910, -0.4500, -0.1561,\n",
      "           0.3926,  0.4045, -0.3430,  0.0588]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prediction,(hidden,cell)  = model_dec(torch.IntTensor([token_ids[-1:],token_ids[-1:]]),hidden_z,cell_z) # Simulate the input of the last token of the sentence\n",
    "print(\"Output shape:\",prediction.shape)\n",
    "print(\"Hidden shape:\",hidden.shape)\n",
    "print(\"Cell shape:\",cell.shape)\n",
    "print(prediction)\n",
    "print(hidden)\n",
    "print(cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is also working fine. It outpus the output from the fully-connected layer twice, one for each token in the target sequence. The same for the hidden and cell states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq model\n",
    "\n",
    "We are now ready to implement the seq2seq model. The seq2seq model will take as input the source sequence and will output the predicted target sequence. \n",
    "\n",
    "Since we want to train with teacher forcing, we will pass the target sequence to the decoder. Teacher forcing is a technique where the target word is passed, with some probability, as the next input to the decoder. The intuition behind teacher forcing is that it will help the decoder learn to better predict the next token.\n",
    "\n",
    "We will use the Encoder and Decoder classes we implemented before. The encoder will take as input the source sequence and will output the context vectors (the last hidden and cell states). Then we will use the Decoder class to iterate over the target sequence and predict the next token. In each iteration we predict the next token only, based in the previous hidden can cell states, and the previous true or predicted token, depending on the teacher forcing probability. \n",
    "\n",
    "Follows the main steps we need to implement:\n",
    "\n",
    "1. Pass the source sequence to the encoder and get the context vectors.\n",
    "2. Initialize the decoder with the context vectors and the \\<sos\\> token.\n",
    "3. Predict the next token, hidden and cell states.\n",
    "4. Repeat 3. with the true or predicted token, depending on the teacher forcing probability, and the new hidden and cell states. \n",
    "5. Stop when we reach the maximum length of the sequence or when we predict the \\<eos\\> token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = source.shape[0]\n",
    "        target_len = target.shape[1]\n",
    "        target_vocab_size = len(tokpt2id)\n",
    "\n",
    "        outputs = torch.zeros(batch_size, target_len, target_vocab_size)\n",
    "\n",
    "        _, (hidden,cell) = self.encoder(source)\n",
    "\n",
    "        # first input to the decoder is the <sos> token\n",
    "        # shape of x: (batch_size, 1)\n",
    "        x = target[:,:1]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "\n",
    "            output, (hidden, cell) = self.decoder(x, hidden, cell)\n",
    "\n",
    "            outputs[:,t:t+1,:] = output\n",
    "\n",
    "            best_guess = output.argmax(dim = -1)\n",
    "\n",
    "            x = target[:,t:t+1] if random.random() < teacher_forcing_ratio else best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(11464, 100)\n",
       "    (rnn): LSTM(100, 256, batch_first=True, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(17557, 100)\n",
       "    (rnn): LSTM(100, 256, batch_first=True, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=256, out_features=17557, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_eng_size = len(tokeng2id)\n",
    "vocab_pt_size = len(tokpt2id)\n",
    "device = \"cuda\"\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "n_layers = 1\n",
    "\n",
    "model_enc = Encoder(vocab_eng_size, embedding_dim, hidden_dim, n_layers)\n",
    "model_dec = Decoder(vocab_pt_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "model = Seq2Seq(model_enc, model_dec).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 8,147,433 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_998/1270215883.py:21: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i, batch in tqdm(enumerate(train_dataloader),total=len(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e732fbdb9e854e41af91a746869a3bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 250 | Train Loss:  6.024 | Val Loss:  5.562 | Train Perplexity:  413.337 | Val Perplexity:  260.410\n",
      "Step: 500 | Train Loss:  5.744 | Val Loss:  5.433 | Train Perplexity:  312.218 | Val Perplexity:  228.794\n",
      "Step: 750 | Train Loss:  5.603 | Val Loss:  5.298 | Train Perplexity:  271.348 | Val Perplexity:  199.883\n",
      "Step: 1000 | Train Loss:  5.499 | Val Loss:  5.217 | Train Perplexity:  244.390 | Val Perplexity:  184.359\n",
      "Step: 1250 | Train Loss:  5.409 | Val Loss:  5.091 | Train Perplexity:  223.455 | Val Perplexity:  162.627\n",
      "Epochs: 1 | Train Loss:  5.409 | Val Loss:  5.091 | Train Perplexity:  223.455 | Val Perplexity:  162.627\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0487dd857244bab81a205882198be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1500 | Train Loss:  4.855 | Val Loss:  4.980 | Train Perplexity:  128.366 | Val Perplexity:  145.423\n",
      "Step: 1750 | Train Loss:  4.807 | Val Loss:  4.924 | Train Perplexity:  122.398 | Val Perplexity:  137.614\n",
      "Step: 2000 | Train Loss:  4.755 | Val Loss:  4.851 | Train Perplexity:  116.141 | Val Perplexity:  127.908\n",
      "Step: 2250 | Train Loss:  4.723 | Val Loss:  4.752 | Train Perplexity:  112.470 | Val Perplexity:  115.851\n",
      "Step: 2500 | Train Loss:  4.683 | Val Loss:  4.733 | Train Perplexity:  108.045 | Val Perplexity:  113.615\n",
      "Epochs: 2 | Train Loss:  4.683 | Val Loss:  4.733 | Train Perplexity:  108.045 | Val Perplexity:  113.615\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a295955ee8a4fb096c87a6ab0361a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2750 | Train Loss:  4.411 | Val Loss:  4.653 | Train Perplexity:  82.373 | Val Perplexity:  104.862\n",
      "Step: 3000 | Train Loss:  4.364 | Val Loss:  4.630 | Train Perplexity:  78.534 | Val Perplexity:  102.558\n",
      "Step: 3250 | Train Loss:  4.337 | Val Loss:  4.590 | Train Perplexity:  76.503 | Val Perplexity:  98.454\n",
      "Step: 3500 | Train Loss:  4.303 | Val Loss:  4.558 | Train Perplexity:  73.938 | Val Perplexity:  95.433\n",
      "Step: 3750 | Train Loss:  4.283 | Val Loss:  4.485 | Train Perplexity:  72.428 | Val Perplexity:  88.716\n",
      "Epochs: 3 | Train Loss:  4.283 | Val Loss:  4.485 | Train Perplexity:  72.428 | Val Perplexity:  88.716\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89bfd9a03a504674a0605848cfe24865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4000 | Train Loss:  4.045 | Val Loss:  4.465 | Train Perplexity:  57.128 | Val Perplexity:  86.913\n",
      "Step: 4250 | Train Loss:  4.029 | Val Loss:  4.432 | Train Perplexity:  56.227 | Val Perplexity:  84.123\n",
      "Step: 4500 | Train Loss:  4.015 | Val Loss:  4.398 | Train Perplexity:  55.432 | Val Perplexity:  81.262\n",
      "Step: 4750 | Train Loss:  4.011 | Val Loss:  4.374 | Train Perplexity:  55.195 | Val Perplexity:  79.339\n",
      "Step: 5000 | Train Loss:  3.998 | Val Loss:  4.336 | Train Perplexity:  54.489 | Val Perplexity:  76.439\n",
      "Epochs: 4 | Train Loss:  3.998 | Val Loss:  4.336 | Train Perplexity:  54.489 | Val Perplexity:  76.439\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138e8dfefbd2456892e559574eebda3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5250 | Train Loss:  3.769 | Val Loss:  4.319 | Train Perplexity:  43.337 | Val Perplexity:  75.134\n",
      "Step: 5500 | Train Loss:  3.760 | Val Loss:  4.308 | Train Perplexity:  42.941 | Val Perplexity:  74.300\n",
      "Step: 5750 | Train Loss:  3.761 | Val Loss:  4.275 | Train Perplexity:  43.012 | Val Perplexity:  71.906\n",
      "Step: 6000 | Train Loss:  3.749 | Val Loss:  4.260 | Train Perplexity:  42.468 | Val Perplexity:  70.795\n",
      "Step: 6250 | Train Loss:  3.734 | Val Loss:  4.244 | Train Perplexity:  41.842 | Val Perplexity:  69.686\n",
      "Epochs: 5 | Train Loss:  3.734 | Val Loss:  4.244 | Train Perplexity:  41.842 | Val Perplexity:  69.686\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc34c91cb6a3478f95d048c000d44b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6500 | Train Loss:  3.550 | Val Loss:  4.228 | Train Perplexity:  34.816 | Val Perplexity:  68.563\n",
      "Step: 6750 | Train Loss:  3.532 | Val Loss:  4.216 | Train Perplexity:  34.198 | Val Perplexity:  67.761\n",
      "Step: 7000 | Train Loss:  3.529 | Val Loss:  4.187 | Train Perplexity:  34.097 | Val Perplexity:  65.845\n",
      "Step: 7250 | Train Loss:  3.525 | Val Loss:  4.192 | Train Perplexity:  33.937 | Val Perplexity:  66.170\n",
      "Step: 7500 | Train Loss:  3.521 | Val Loss:  4.161 | Train Perplexity:  33.803 | Val Perplexity:  64.131\n",
      "Epochs: 6 | Train Loss:  3.521 | Val Loss:  4.161 | Train Perplexity:  33.803 | Val Perplexity:  64.131\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09eec31cbab34ebda5cad92e14d0479c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7750 | Train Loss:  3.324 | Val Loss:  4.159 | Train Perplexity:  27.783 | Val Perplexity:  63.996\n",
      "Step: 8000 | Train Loss:  3.317 | Val Loss:  4.181 | Train Perplexity:  27.575 | Val Perplexity:  65.439\n",
      "Step: 8250 | Train Loss:  3.321 | Val Loss:  4.162 | Train Perplexity:  27.677 | Val Perplexity:  64.181\n",
      "Step: 8500 | Train Loss:  3.323 | Val Loss:  4.122 | Train Perplexity:  27.741 | Val Perplexity:  61.665\n",
      "Step: 8750 | Train Loss:  3.322 | Val Loss:  4.096 | Train Perplexity:  27.723 | Val Perplexity:  60.108\n",
      "Epochs: 7 | Train Loss:  3.322 | Val Loss:  4.096 | Train Perplexity:  27.723 | Val Perplexity:  60.108\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5c577399904d51b37b4ad4c815fd56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 9000 | Train Loss:  3.120 | Val Loss:  4.117 | Train Perplexity:  22.636 | Val Perplexity:  61.404\n",
      "Step: 9250 | Train Loss:  3.138 | Val Loss:  4.096 | Train Perplexity:  23.059 | Val Perplexity:  60.102\n",
      "Step: 9500 | Train Loss:  3.142 | Val Loss:  4.118 | Train Perplexity:  23.141 | Val Perplexity:  61.424\n",
      "Step: 9750 | Train Loss:  3.147 | Val Loss:  4.092 | Train Perplexity:  23.263 | Val Perplexity:  59.844\n",
      "Step: 10000 | Train Loss:  3.146 | Val Loss:  4.068 | Train Perplexity:  23.232 | Val Perplexity:  58.453\n",
      "Epochs: 8 | Train Loss:  3.146 | Val Loss:  4.068 | Train Perplexity:  23.232 | Val Perplexity:  58.453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c39dd67789149f0a4cdfa75235a0ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10250 | Train Loss:  2.976 | Val Loss:  4.090 | Train Perplexity:  19.600 | Val Perplexity:  59.752\n",
      "Step: 10500 | Train Loss:  2.977 | Val Loss:  4.085 | Train Perplexity:  19.632 | Val Perplexity:  59.427\n",
      "Step: 10750 | Train Loss:  2.988 | Val Loss:  4.053 | Train Perplexity:  19.838 | Val Perplexity:  57.592\n",
      "Step: 11000 | Train Loss:  2.990 | Val Loss:  4.055 | Train Perplexity:  19.890 | Val Perplexity:  57.670\n",
      "Step: 11250 | Train Loss:  2.992 | Val Loss:  4.060 | Train Perplexity:  19.922 | Val Perplexity:  57.992\n",
      "Epochs: 9 | Train Loss:  2.992 | Val Loss:  4.060 | Train Perplexity:  19.922 | Val Perplexity:  57.992\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152bd6eaa719472f8c6e43967a67e320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 11500 | Train Loss:  2.800 | Val Loss:  4.054 | Train Perplexity:  16.444 | Val Perplexity:  57.607\n",
      "Step: 11750 | Train Loss:  2.817 | Val Loss:  4.074 | Train Perplexity:  16.733 | Val Perplexity:  58.799\n",
      "Step: 12000 | Train Loss:  2.828 | Val Loss:  4.056 | Train Perplexity:  16.911 | Val Perplexity:  57.734\n",
      "Step: 12250 | Train Loss:  2.838 | Val Loss:  4.036 | Train Perplexity:  17.077 | Val Perplexity:  56.599\n",
      "Step: 12500 | Train Loss:  2.849 | Val Loss:  4.023 | Train Perplexity:  17.263 | Val Perplexity:  55.870\n",
      "Epochs: 10 | Train Loss:  2.849 | Val Loss:  4.023 | Train Perplexity:  17.263 | Val Perplexity:  55.870\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import math\n",
    "\n",
    "# main training loop\n",
    "n_epochs = 10\n",
    "lr=1e-3\n",
    "clip = 1\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "step = 0\n",
    "evaluation_step = 250\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss_train_total = 0\n",
    "\n",
    "\n",
    "    for i, batch in tqdm(enumerate(train_dataloader),total=len(train_dataloader)):\n",
    "        step += 1\n",
    "\n",
    "        source = batch['english'].to(device)\n",
    "        target = batch['portuguese'].to(device)\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        output = model(source, target)\n",
    "        output = output.to(device)\n",
    "\n",
    "        output = output[:,1:,:].reshape(-1, output.shape[2])\n",
    "        target = target[:,1:].reshape(-1)\n",
    "\n",
    "        # target = target.to(\"cpu\")\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss_train_total += loss.item()\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the gradients to prevent exploding gradient problem. This step is very important.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(loss.item())\n",
    "\n",
    "        # evaluation step\n",
    "        if step % evaluation_step == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # evaluate on training data\n",
    "                loss_val = 0\n",
    "                for j,batch in enumerate(val_dataloader):\n",
    "                    source = batch['english'].to(device)\n",
    "                    target = batch['portuguese'].to(device)\n",
    "\n",
    "                    # forward pass\n",
    "                    output = model(source, target, teacher_forcing_ratio = 0) # we do not use teacher forcing here\n",
    "                    output = output.to(device)\n",
    "\n",
    "                    output = output[:,1:,:].reshape(-1, output.shape[2])\n",
    "                    target = target[:,1:].reshape(-1)\n",
    "\n",
    "                    loss = criterion(output, target)\n",
    "                    loss_val += loss.item()\n",
    "\n",
    "\n",
    "            # Calculate training perplexity\n",
    "            train_perplexity = math.exp(loss_train_total/i)\n",
    "\n",
    "\n",
    "            # Calculate perplexity\n",
    "            val_perplexity = math.exp(loss_val / j)\n",
    "\n",
    "\n",
    "            # print the loss at each step \n",
    "            print(f'Step: {step} | Train Loss: {loss_train_total/i: .3f} | Val Loss: {loss_val/j: .3f} | Train Perplexity: {train_perplexity: .3f} | Val Perplexity: {val_perplexity: .3f}')\n",
    "\n",
    "            model.train()\n",
    "            \n",
    "            train_losses.append(loss_train_total/i)\n",
    "            val_losses.append(loss_val/j)\n",
    "\n",
    "    \n",
    "    # print the loss and ppl at each epoch\n",
    "    print(f'Epochs: {epoch + 1} | Train Loss: {loss_train_total/i: .3f} | Val Loss: {loss_val/j: .3f} | Train Perplexity: {train_perplexity: .3f} | Val Perplexity: {val_perplexity: .3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now let's test it by generating a translation from english to portuguese, by building a function that receives a sentence in english and outputs the predicted translation in portuguese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text,max_len = 30):\n",
    "    tokens = word_tokenize(text.lower(), language='english')\n",
    "    tokens = ['<sos>'] + tokens + ['<eos>']\n",
    "    tokens_ids = [tokeng2id[t] for t in tokens]\n",
    "    tokens_tensor = torch.LongTensor(tokens_ids).unsqueeze(0).to(device)\n",
    "    target_tensor = torch.LongTensor([1]*max_len).unsqueeze(0).to(device) # We just need a tensor with defined max length. Except the first token, the other tokens are not important, since we will not use teacher forcing.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(tokens_tensor,target_tensor,teacher_forcing_ratio=0)\n",
    "        for i in range(1,max_len):\n",
    "            predicted_id = predictions[0,i,:].argmax(dim=-1).item()\n",
    "            if predicted_id == 2: # <eos>   \n",
    "                break\n",
    "            print(id2tokpt[predicted_id],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oi , voc est voc ? "
     ]
    }
   ],
   "source": [
    "translate(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oi , o que o nome ? ? "
     ]
    }
   ],
   "source": [
    "translate(\"Hello, what is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'english': 'I will be back soon.', 'portuguese': 'Volto j.'},\n",
       " 'id': '21'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data['train'][21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voltarei logo logo . "
     ]
    }
   ],
   "source": [
    "translate(\"I will be back soon.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model is already getting some words right, but it is still far from perfect. \n",
    "\n",
    "We must note however, that this is a simple model with a single layer, with just 8 million parameters , and trained in a very small dataset. For comparision, the original [paper](https://arxiv.org/pdf/1409.3215.pdf) that proposed this model used a 4 layer LSTM with 1000 hidden units in each layer, with  384 million parameters, and trained in a dataset with 12 million sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.GRU((32 * 2) + 100, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
